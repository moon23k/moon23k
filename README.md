### <img src="https://emojis.slackmojis.com/emojis/images/1531849430/4246/blob-sunglasses.gif?1531849430" width="30"/> Hi there üëã
I'm moon, Machine Learning Engineer

I'm interested in NLP, especially Text Generation Tasks like NMT, ChatBot, Summarization and so on.

</br>
</br>



### Projects
| **Name** | **Desc** | **Key Words** |
|------|------|--------|
| [NLG_Basics](https://github.com/moon23k/NLG_Basics) | Implements Basic NLG Model Architectures from scratch and Compares performances | Seq2Seq, Attention Mechanism, Transformer, NMT, Dialogue Genreration |
| [Light Transformer](https://github.com/moon23k/Light_Transformer) | Compare Performance and Speed between Vanila Transformer and Light-Weighted Transformer | Transformer, Weight-Sharing, Parameter Factorizing, Model Lightening |
| [NLG_BERT](https://github.com/moon23k/NLG_BERT) | Apply Pre-trained BERT Model to NLG Tasks | BERT, NMT, Dialogue Genreration |
| [NLG_KoBERT](https://github.com/moon23k/NMT_KoBERT) | Compare KoBERT and Multi-Lingual Bert Models on Korean NLG datasets | KoBERT, BERT_multilingual, NMT, Dialogue Genreration |
| [NLG_BERTs](https://github.com/moon23k/NMT_Bert) | Apply BERT based Pre-trained Auto Encoding Models to NLG Tasks and compare performance with Basic BERT Model | BERT, AlBERT, XLNet, RoBERTa, ELECTRA, NMT, Dialogue Genreration |
| [SumNMT](https://github.com/moon23k/SumNMT) | Summarize Long English Text into Shorter Sequences, and then Translate them to Korean | BERT, Text Summarization, NMT |
| [NLG_GAN](https://github.com/moon23k/seqGAN) | Train Generative ChatBot Model by using concepts from GAN and techniques from Reinforcement learning | GAN, Reinforcement Learning, Policy Graident |
| [C2 Bot](https://github.com/moon23k/C2_Bot) | Characteristic Chat Bot(C2 Bot), tries to inject Characteristic to ChatBot Model with seqGAN architecture | Reinforcement Learning, SeqGAN, Text Generation |
| [C3 Bot](https://github.com/moon23k/C3_Bot) | Consistent Characteristic Chat Bot (C3 Bot), upgraded version of C2 Bot. The model seeks to Consistency on Multi-Turn Dialogue Task. | Reinforcement Learning, SeqGAN, Text Generation, P^2Bot, Multi-Turn Dialogue Generation |
| Distil_Transformer | Knowledge Distilled Transformer, Distilled from BERT and GPT | Knowledge Distillation, Transformer, BERT, GPT |
| Enhanced_Transformer | Enhance Transformer Model via various Encoding and Decoding Strategies | Transformer, Poly Encoder, Beam Search, Random Sampling Decoding |
| [NLP Datasets](https://github.com/moon23k/NLP_datasets) | Codes to Get and Process Training Datasets for various NLP Tasks | Neural Machine Translation, Dialogue Generation, Summarization |

<br>




## Projects
<details>	
  <summary><b> Machine Translation </b></summary><br>
  <table>
    <thead align="center">
      <tr border: none;>
        <td><b>üíª Repo</b></td>
        <td><b>üåü Desc</b></td>
        <td><b>üç¥ Dataset</b></td>
        <td><b>üêõ Architecture</b></td>
      </tr>
    </thead>
    <tbody>
      <tr>
	<td><a href="https://github.com/iampavangandhi/Gitwar"><b>NMT_Basics</b></a></td>
        <td>Basic Architectures for machine Translation</td>
        <td>WMT-14(De-En)</td>
        <td>Sequence-to-Sequence, Attention Mechanism, Transformer</td>
      </tr>
      <tr>
	<td><a href="https://github.com/iampavangandhi/TradeByte"><b>NMT_BERT</b></a></td>
        <td>Incorporate BERT into NMT Task</td>
        <td>WMT-14(De-En)</td>
        <td>Sequence-to-Sequence, BERT, Transformer</td>
      </tr>
      <tr>
	<td><a href="https://github.com/iampavangandhi/TheNodeCourse"><b>NMT_KoBERT</b></a></td>
        <td>Incorporate KoBERT into NMT Task and compare it with BERT-Multilingual</td>
        <td>Korean-English Translation Dataset(AI Hub)</td>
        <td>Sequence-to-Sequence, BERT, KoBERT, Transformer</td>
      </tr>
      <tr>
	<td><a href="https://github.com/iampavangandhi/iampavangandhi"><b>NMT_GAN</b></a></td>
        <td>Apply Adversarial Training via RL technique</td>
        <td>WMT-14(De-En)</td>
        <td>Sequence-to-Sequence, BERT, Transformer</td>
      </tr>
    </tbody>
  </table>
  <br />
</details>


<details>	
  <summary><b> Abstractive Text Summarization </b></summary><br>
  <table>
    <thead align="center">
      <tr border: none;>
        <td><b>üíª Repo</b></td>
        <td><b>üåü Desc</b></td>
        <td><b>üç¥ Dataset</b></td>
        <td><b>üêõ Architecture</b></td>
      </tr>
    </thead>
    <tbody>
      <tr>
	<td><a href="https://github.com/iampavangandhi/Gitwar"><b>Hier_Transformer</b></a></td>
        <td>Basic Architectures for machine Translation</td>
        <td>WMT-14(De-En)</td>
        <td>Sequence-to-Sequence, Attention Mechanism, Transformer</td>
      </tr>
      <tr>
	<td><a href="https://github.com/iampavangandhi/TradeByte"><b>Hier_BERT</b></a></td>
        <td>Incorporate BERT into NMT Task</td>
        <td>WMT-14(De-En)</td>
        <td>Sequence-to-Sequence, BERT, Transformer</td>
      </tr>
      <tr>
	<td><a href="https://github.com/iampavangandhi/TheNodeCourse"><b>SUM_GAN</b></a></td>
        <td>Incorporate KoBERT into NMT Task and compare it with BERT-Multilingual</td>
        <td>Korean-English Translation Dataset(AI Hub)</td>
        <td>Sequence-to-Sequence, BERT, KoBERT, Transformer</td>
      </tr>
      <tr>
	<td><a href="https://github.com/iampavangandhi/iampavangandhi"><b>SUM_NMT</b></a></td>
        <td>Apply Adversarial Training via RL technique</td>
        <td>WMT-14(De-En)</td>
        <td>Sequence-to-Sequence, BERT, Transformer</td>
      </tr>
    </tbody>
  </table>
  <br />
</details>


<details>	
  <summary><b> Dialogue Generation </b></summary><br>
  <table>
    <thead align="center">
      <tr border: none;>
        <td><b>üíª Repo</b></td>
        <td><b>üåü Desc</b></td>
        <td><b>üç¥ Dataset</b></td>
        <td><b>üêõ Architecture</b></td>
      </tr>
    </thead>
    <tbody>
      <tr>
	<td><a href="https://github.com/iampavangandhi/Gitwar"><b>Chat_Basics</b></a></td>
        <td>Basic Architectures for machine Translation</td>
        <td>WMT-14(De-En)</td>
        <td>Sequence-to-Sequence, Attention Mechanism, Transformer</td>
      </tr>
      <tr>
	<td><a href="https://github.com/iampavangandhi/TradeByte"><b>Chat_BERT</b></a></td>
        <td>Incorporate BERT into NMT Task</td>
        <td>WMT-14(De-En)</td>
        <td>Sequence-to-Sequence, BERT, Transformer</td>
      </tr>
      <tr>
	<td><a href="https://github.com/iampavangandhi/TheNodeCourse"><b>C2_Bot</b></a></td>
        <td>Characteristic Chat (C2) Bot via GAN</td>
        <td>HIMYM Transcripts</td>
        <td>Sequence-to-Sequence, BERT, KoBERT, Transformer</td>
      </tr>
      <tr>
	<td><a href="https://github.com/iampavangandhi/iampavangandhi"><b>C3_Bot</b></a></td>
        <td>Coherent Characteristic Chat (C3) Bot</td>
        <td>WMT-14(De-En)</td>
        <td>Sequence-to-Sequence, BERT, Transformer</td>
      </tr>
    </tbody>
  </table>
  <br />
</details>


<details>	
  <summary><b> Ablation Studies </b></summary><br>
  <table>
    <thead align="center">
      <tr border: none;>
        <td><b>üíª Repo</b></td>
        <td><b>üåü Desc</b></td>
        <td><b>üç¥ Dataset</b></td>
        <td><b>üêõ Architecture</b></td>
      </tr>
    </thead>
    <tbody>
      <tr>
	<td><a href="https://github.com/iampavangandhi/Gitwar"><b>Transformer_Ablation</b></a></td>
        <td>Basic Architectures for machine Translation</td>
        <td>WMT-14(De-En)</td>
        <td>Sequence-to-Sequence, Attention Mechanism, Transformer</td>
      </tr>
      <tr>
	      <td><a href="https://github.com/iampavangandhi/TradeByte"><b>BERT_Ablation</b></a></td>
        <td>Incorporate BERT into NMT Task</td>
        <td>WMT-14(De-En)</td>
        <td>Sequence-to-Sequence, BERT, Transformer</td>
      </tr>
      <tr>
	<td><a href="https://github.com/iampavangandhi/TheNodeCourse"><b>PipiLine_Ablation</b></a></td>
        <td>Incorporate KoBERT into NMT Task and compare it with BERT-Multilingual</td>
        <td>WMT-14(De-En)</td>
        <td>Transformer</td>
      </tr>
      <tr>
	<td><a href="https://github.com/iampavangandhi/iampavangandhi"><b>Enc_Dec_Ablation</b></a></td>
        <td>Encoding and Decoding Techniques</td>
        <td>WMT-14(De-En)</td>
        <td>Sequence-to-Sequence, Transformer</td>
      </tr>
    </tbody>
  </table>
  <br />
</details>


<details>	
  <summary><b> Model Lightening </b></summary><br>

| **Name** | **Desc** | **Key Words** |
|------|------|--------|
| [NLG_Basics](https://github.com/moon23k/NLG_Basics) | Implements Basic NLG Model Architectures from scratch and Compares performances | Seq2Seq, Attention Mechanism, Transformer, NMT, Dialogue Genreration |
| [Light Transformer](https://github.com/moon23k/Light_Transformer) | Compare Performance and Speed between Vanila Transformer and Light-Weighted Transformer | Transformer, Weight-Sharing, Parameter Factorizing, Model Lightening |
| [NLG_BERT](https://github.com/moon23k/NLG_BERT) | Apply Pre-trained BERT Model to NLG Tasks | BERT, NMT, Dialogue Genreration |
| [NLG_KoBERT](https://github.com/moon23k/NMT_KoBERT) | Compare KoBERT and Multi-Lingual Bert Models on Korean NLG datasets | KoBERT, BERT_multilingual, NMT, Dialogue Genreration |
  <br />
</details>


<details>
  <summary><b> Datasets </b></summary>

* **[NMT_Basics](https://github.com/moon23k/NLG_Basics)**: Implements Basic NLG Model Architectures from scratch and Compares performances<br>

* **[NMT_BERT](https://github.com/moon23k/NLG_Basics)**: Implements Basic NLG Model Architectures from scratch and Compares performances<br>

* **[NMT_KoBERT](https://github.com/moon23k/NLG_Basics)**: Implements Basic NLG Model Architectures from scratch and Compares performances<br>

* **[NMT_GAN](https://github.com/moon23k/NLG_Basics)**: Implements Basic NLG Model Architectures from scratch and Compares performances<br>
  <br />
</details>


<br>
<br>

### Projects
<table><tr><td valign="top">

### üî• Machine Translation
* **[NMT_Basics](https://github.com/moon23k/NLG_Basics)**<br>
* **[NMT_BERT](https://github.com/moon23k/NLG_Basics)**<br>
* **[NMT_KoBERT](https://github.com/moon23k/NLG_Basics)**<br>
* **[NMT_GAN](https://github.com/moon23k/NLG_Basics)**
</td><td valign="top">

### üî• Abstract Summarization
* **[Hier_Transformer](https://github.com/moon23k/NLG_Basics)**<br>
* **[Hier_BERT](https://github.com/moon23k/NLG_Basics)**<br>
* **[Sum_GAN](https://github.com/moon23k/NLG_Basics)**<br>
* **[Sum_NMT](https://github.com/moon23k/NLG_Basics)**
</td></tr>

<tr><td valign="top">

### üî• Chat Bot
* **[Chat_Basics](https://github.com/moon23k/NLG_Basics)**<br>
* **[Chat_BERT](https://github.com/moon23k/NLG_Basics)**<br>
* **[C2_Bot](https://github.com/moon23k/NLG_Basics)**<br>
* **[C3_Bot](https://github.com/moon23k/NLG_Basics)**
</td><td valign="top">

### üî• Ablation Studies
* **[Transformer_Ablations](https://github.com/moon23k/NLG_Basics)**<br>
* **[BERT_Ablations](https://github.com/moon23k/NLG_Basics)**<br>
* **[PipeLine_Ablations](https://github.com/moon23k/NLG_Basics)**<br>
* **[Enc_Dec_Ablations](https://github.com/moon23k/NLG_Basics)**
</td></tr>

<tr><td valign="top">

### üî• Model Lightening
* **[NMT_Basics](https://github.com/moon23k/NLG_Basics)**<br>
* **[NMT_BERT](https://github.com/moon23k/NLG_Basics)**<br>
* **[NMT_Basics](https://github.com/moon23k/NLG_Basics)**<br>
* **[NMT_BERT](https://github.com/moon23k/NLG_Basics)**
</td><td valign="top">

### üî• Dataset
* **[NMT_Basics](https://github.com/moon23k/NLG_Basics)**<br>
* **[NMT_BERT](https://github.com/moon23k/NLG_Basics)**<br>
* **[NMT_Basics](https://github.com/moon23k/NLG_Basics)**<br>
* **[NMT_BERT](https://github.com/moon23k/NLG_Basics)**
</td></tr>
</table>

<br>
<br>


<br>
<br>


### Github Stats  
<table><tr><td valign="top" width="50%">

<img src="https://github-readme-stats.vercel.app/api?username=moon23k&show_icons=true&count_private=true&hide_border=true" align="left" style="width: 100%" />

</td><td valign="top" width="50%">
  
<img src="https://github-readme-stats.vercel.app/api/top-langs/?username=moon23k&hide_border=true&layout=compact" align="left" style="width: 100%" />

</td></tr></table>  
