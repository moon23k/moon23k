<br> 

## ✋ Hello World
&nbsp; Hi there! I'm **moon**, someone dedicated to tackling challenges through artificial intelligence. Among the various AI subcategories, Natural Language Processing intrigues me the most. By profession, I'm a **NLP Machine Learning Engineer**. My goal as an engineer is to create models that can engage in natural communication with people. The code in all my repositories reflects my progress toward achieving this objective. In addition to the codes in my git repos, I've provided comprehensive project summaries on my <a href="https://moon23k.github.io/">portfolio website</a>. I encourage you to take a moment to explore and discover more about each project. If you'd like to get in touch, feel free to reach out to me via the email address listed in the left information field.

<br><br> 


## 🤖&hairsp; Architecture 
> &nbsp; The model architecture plays a pivotal role in machine learning engineering, as it can greatly influence performance.
Below, you'll find a collection of projects focused on exploring and establishing standards for appropriate model structures in three NLG tasks: Translation, Dialogue Generation, and Summarization.

&emsp; [**•&hairsp; RNN Seq2Seq**](https://github.com/moon23k/RNN_Seq2Seq) &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &ensp; &emsp; 
       [**•&hairsp; RNN Seq2Seq with Attention**](https://github.com/moon23k/RNN_Seq2Seq_Attention) &emsp; &emsp; &emsp; &emsp; &emsp; 
       [**•&hairsp; Transformer**](https://github.com/moon23k/Transformer) <br> 
       
&emsp; [**•&hairsp; Transformer Balance**](https://github.com/moon23k/Transformer_Balance) &emsp; &emsp; &emsp; &emsp; &emsp; 
       [**•&hairsp; Transformer Variants**](https://github.com/moon23k/Transformer_Variants) &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &ensp; &hairsp; 
       [**•&hairsp; Transformer Fusion**](https://github.com/moon23k/Transformer_Fusion)
       
<br><br> 


## 🏃‍♂️&hairsp; Training Strategy
> &nbsp; Alongside model architecture, another crucial factor influencing the performance of deep learning models is the **training strategy**. In order to explore more advanced training methodologies, various approaches such as pretraining, finetuning, GANs, etc., are being applied and developed, alongside diverse research endeavors including deep studies aimed at enhancing efficiency. 

&emsp; [**•&hairsp; Generation&hairsp; Improveing&hairsp; Fine-Tuning**](https://github.com/moon23k/GIFT) &emsp; &emsp; &emsp; 
       [**•&hairsp; SlowGAN**](https://github.com/moon23k/SlowGAN) &emsp; &emsp; &emsp; 
       [**•&hairsp; Customized&hairsp; Pretraining**](https://github.com/moon23k/Customized_Pretraining) &emsp; &emsp; &emsp; 
       [**•&hairsp; Efficient&hairsp; Training**](https://github.com/moon23k/Efficient_Training)

<br><br> 


## 🎯&hairsp; Task Specific 
> &nbsp; Machine translation is the task of converting Text from Source Language into Target Language using a computer processing. The hegemony of machine translation was Rule-Based at the earliest, followed by SMT, and now NMT has been established. NMT aims to derive more accurate and natural translation results using Neural Networks. Below are experiments of various Neural Network Architectures for this purpose.

&emsp; [**•&hairsp; Multi-Lingual Translation**](https://github.com/moon23k/NMT_MultiLingual) &emsp; &emsp; &emsp; &ensp; &nbsp; &thinsp; 
       [**•&hairsp; Code Translation**](https://github.com/moon23k/NMT_Code) &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; 
       [**•&hairsp; Characteristic Dialogue Generation**](https://github.com/moon23k/Dialog_Char) <br>  
       
&emsp; [**•&hairsp; Diverse Dialogue Generation**](https://github.com/moon23k/Dialog_SemEnt) &emsp; &emsp; &emsp; 
       [**•&hairsp; Hierarchical Summarization**](https://github.com/moon23k/Sum_Hierarchical) &emsp; &emsp; &emsp; 
       [**•&hairsp; Sparse Attention Summarization**](https://github.com/moon23k/Sum_Sparse) 

<br><br> 


## 💾&hairsp; Dataset 
> &nbsp; Machine translation is the task of converting Text from Source Language into Target Language using a computer processing. The hegemony of machine translation was Rule-Based at the earliest, followed by SMT, and now NMT has been established. NMT aims to derive more accurate and natural translation results using Neural Networks. Below are experiments of various Neural Network Architectures for this purpose.

&emsp; [**•&hairsp; NLP Datasets**](https://github.com/moon23k/NLP_Datasets) &emsp; &emsp; &emsp; &emsp; 
       [**•&hairsp; Tokenizers**](https://github.com/moon23k/Tokenizers) &emsp; &emsp; &emsp; &emsp; 
       [**•&hairsp; Back Translation**](https://github.com/moon23k/BackTranslation) 
<br> 
