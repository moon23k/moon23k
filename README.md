## <img src="https://emojis.slackmojis.com/emojis/images/1531849430/4246/blob-sunglasses.gif?1531849430" width="30"/> Hello World
&nbsp; Hi there! I'm **moon**, somebody who's focusing on solving problems through artificial intelligence. AI has many subcategories, of which I find Natural Language Processing the most interesting. By profession, I'm a **NLP Machine Learning Engineer**. As an engineer, I aim to develop a model that can communicate naturally with people. So the codes in all my repos will contain the progress towards the goal. In addition to the codes in my git repos, reviews of the papers and personal research on artificial intelligence techniques are recorded on my <a href="https://shy-vole-f74.notion.site/Hello-I-m-moon-e1ecc2e40b32405e997713cfb44e4f3c">notion page</a>. If you would like to contact me, please contact me at the email address in the left information field.

<br><br>

## üë®üèª‚Äçüî¨ NLG Experiments
&nbsp; AI has a lot of variables, which means that various variable settings can make different results. Therefore, many experiments from various viewpoints are required to obtain good results. Below is a series of experiments that introduce different approaches on three representative NLP tasks. Each is **Neural Machine Translation**, **Dialogue Generation**, and **Abstractive Summarization**.

<br>

**Neural Machine Translation**
> Machine translation is the task of converting Text from Source Language into Target Language using a computer processing. The hegemony of machine translation was Rule-Based at the earliest, followed by SMT, and now NMT has been established. NMT aims to derive more accurate and natural translation results using Neural Networks. Below are experiments of various Neural Network Architectures for this purpose.

&emsp; ‚Ä¢ &hairsp; <a href="https://github.com/moon23k/NMT_GEN">**Generative Training** Study for Machine Translation</a>
&emsp; &emsp; &emsp; 
‚Ä¢ &hairsp; <a href="https://github.com/moon23k/NMT_GAN">Utilize **SeqGAN** Technique on Machine Translation</a>

&emsp; ‚Ä¢ &hairsp; <a href="https://github.com/moon23k/NMT_Back">**Back Translation** Utilization Study</a>
&emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &nbsp; &thinsp;
‚Ä¢ &hairsp; <a href="https://github.com/moon23k/NMT_BERT">Utilize Pretrained **BERT** on Machine Translation</a>

<br>

**Dialogue Generation**
> Dialogue Generation is a task to generate a response to a previous utterance, just like humans do in a conversational situation. However, it is very difficult for the model to understand the flow of the conversation and return appropriate answers. Below is a set of experiments to generate more natural responses like humans do.

&emsp; ‚Ä¢ &hairsp; <a href="https://github.com/moon23k/Dialog_GEN">**Generative Training** Study for Dialogue Generation</a>
&emsp; &emsp; &emsp; 
‚Ä¢ &hairsp; <a href="https://github.com/moon23k/Dialog_SemEnt">Utilize **SimEnt** Technique to boost up Diversity</a>

&emsp;  ‚Ä¢ &hairsp; <a href="https://github.com/moon23k/Dialog_GAN">Utilize **SeqGAN** Technique on Dialogue Generation</a>
&emsp; &emsp; &emsp; &hairsp; 
‚Ä¢ &hairsp; <a href="https://github.com/moon23k/Dialog_Char">How to give **Characteristic** on Dialogue Generation Model</a>

&emsp; ‚Ä¢ &hairsp; <a href="https://github.com/moon23k/Dialog_MulT">**Multi-Turn Dialgue Generation** Study</a>

<br>

**Abstract Text Summarization**
> Summarization Task summarizes long text into short sentences through Neural Networks, and the task can be devided into Extractive and Abstractive methods. Extractive Summarization selects key sentences from original text to make summary, whereas Abstractive Summarization creates a new summary sentence through the model's decoder. The experiments below mainly deal with Abstractive summary tasks.

&emsp; ‚Ä¢ &hairsp; <a href="https://github.com/moon23k/Sum_Encoders">**Encoder** focused study for Text Summarization</a>
&emsp; &emsp; &emsp; 
‚Ä¢ &hairsp; <a href="https://github.com/moon23k/Sum_GAN">Utilize **SeqGAN** Technique on Text Summarization</a>

&emsp; ‚Ä¢ &hairsp; <a href="https://github.com/moon23k/Sum_BERT">Utilize Pretrained **BERT** on Text Summarization</a> 
&emsp; &emsp; &emsp; 
‚Ä¢ &hairsp; <a href="https://github.com/moon23k/Sum_Sparse">**Sparse Attention** Comparision Study</a>

<br><br>

## ‚öì Anchor Code
&nbsp; AI research has been actively conducted, is currently in progress, and there will be more and more in the future. As research becomes more diverse, a baseline for objective evaluation is essential. I set four baselines for the NLG Tasks and call them **Anchor Code**, each is **RNN**, **Attention Mechanism**, **Transformer**. 

&emsp; ‚Ä¢ &hairsp; <a href="https://github.com/moon23k/RNN_Anchors">**RNN** &hairsp; Anchor Codes</a> 
&emsp; &emsp; &emsp; &emsp; 
‚Ä¢ &hairsp; <a href="https://github.com/moon23k/Attention_Anchors">**Attention** &hairsp; Anchor Codes</a> 
&emsp; &emsp; &emsp; &emsp; 
‚Ä¢ &hairsp; <a href="https://github.com/moon23k/Transformer_Anchors">**Transformer** &hairsp; Anchor Codes</a>

<br><br>

## üìÑ Comparative Analysis
&nbsp; AI models show different results depending on the architecture and different techniques. Even a small change can make a big difference, so building an ability to control the change is necessary. Of course, this requires a lot of experiments, and below are the experiments.

&emsp; ‚Ä¢ &hairsp; <a href="https://github.com/moon23k/Aux_Training">**Auxiliary Training**</a>
&emsp; &emsp; &nbsp; &emsp; 
‚Ä¢ &hairsp; <a href="https://github.com/moon23k/Transformer_Arhcs">**Transformer Architectures**</a>
&emsp; &emsp; &emsp; &ensp; &hairsp; 
‚Ä¢ &hairsp; <a href="https://github.com/moon23k/PipeLines">**Module PipeLines**</a> 

&emsp; ‚Ä¢ &hairsp; <a href="https://github.com/moon23k/Efficient_Models">**Lightweight PLMs**</a> 
&emsp; &emsp; &emsp; &hairsp; 
‚Ä¢ &hairsp; <a href="https://github.com/moon23k/Efficient_Training">**Efficient Training Strategies**</a> 
&emsp; &emsp; &emsp; &hairsp; 
‚Ä¢ &hairsp; <a href="https://github.com/moon23k/PEFT">**PEFT**</a>

<br><br>

## üíæ Data
&nbsp; Deep Learning model learns from large amounts of data. Therefore, high-quality bulk data is essential for building a good model. Below is a series of Data-Related Experiments. And the Experiements include from simply fetching and processing data, to further research such as Tokenization and Data Augmentation.

&emsp; ‚Ä¢ &hairsp; <a href="https://github.com/moon23k/NLP_Datasets">**Data Load and Process**</a> &emsp; &emsp; &emsp;
‚Ä¢ &hairsp; <a href="https://github.com/moon23k/Tokenizations">**Tokenization**</a> &emsp; &emsp; &emsp;
‚Ä¢ &hairsp; <a href="https://github.com/moon23k/Data_Augmentation">**Data Augmentation**</a>

<br>
