---

### <img src="https://emojis.slackmojis.com/emojis/images/1531849430/4246/blob-sunglasses.gif?1531849430" width="30"/> Hi there
I'm moon, Machine Learning Engineer

I'm interested in NLP, especially Text Generation Tasks like NMT, ChatBot, Summarization and so on.

</br>
</br>


## Projects
<details>	
  <summary><b> ‚≠ï Machine Translation </b></summary><br>
  <table>
    <thead align="center">
      <tr border: none;>
        <td><b>üíª Repo</b></td>
        <td><b>üí° Desc</b></td>
        <td><b>üíæ Dataset</b></td>
        <td><b>‚öôÔ∏è Architecture</b></td>
      </tr>
    </thead>
    <tbody>
      <tr>
	<td><a href="https://github.com/iampavangandhi/Gitwar"><b>NMT_Basics</b></a></td>
        <td>Basic Architectures for machine Translation</td>
        <td>WMT-14(De-En)</td>
        <td>Sequence-to-Sequence, Attention Mechanism, Transformer</td>
      </tr>
      <tr>
	<td><a href="https://github.com/iampavangandhi/TradeByte"><b>NMT_BERT</b></a></td>
        <td>Incorporate BERT into NMT Task</td>
        <td>WMT-14(De-En)</td>
        <td>Sequence-to-Sequence, BERT, Transformer</td>
      </tr>
      <tr>
	<td><a href="https://github.com/iampavangandhi/TheNodeCourse"><b>NMT_KoBERT</b></a></td>
        <td>Incorporate KoBERT into NMT Task and compare it with BERT-Multilingual</td>
        <td>Korean-English Translation Dataset(AI Hub)</td>
        <td>Sequence-to-Sequence, BERT, KoBERT, Transformer</td>
      </tr>
      <tr>
	<td><a href="https://github.com/iampavangandhi/iampavangandhi"><b>NMT_GAN</b></a></td>
        <td>Apply Adversarial Training via RL technique</td>
        <td>WMT-14(De-En)</td>
        <td>Sequence-to-Sequence, BERT, Transformer</td>
      </tr>
    </tbody>
  </table>
  <br />
</details>


<details>	
  <summary><b> ‚≠ï Abstractive Text Summarization </b></summary><br>
  <table>
    <thead align="center">
      <tr border: none;>
        <td><b>üíª Repo</b></td>
        <td><b>üí° Desc</b></td>
        <td><b>üíæ Dataset</b></td>
        <td><b>‚öôÔ∏è Architecture</b></td>
      </tr>
    </thead>
    <tbody>
      <tr>
	<td><a href="https://github.com/iampavangandhi/Gitwar"><b>Hier_Transformer</b></a></td>
        <td>Basic Architectures for machine Translation</td>
        <td>WMT-14(De-En)</td>
        <td>Sequence-to-Sequence, Attention Mechanism, Transformer</td>
      </tr>
      <tr>
	<td><a href="https://github.com/iampavangandhi/TradeByte"><b>Hier_BERT</b></a></td>
        <td>Incorporate BERT into NMT Task</td>
        <td>WMT-14(De-En)</td>
        <td>Sequence-to-Sequence, BERT, Transformer</td>
      </tr>
      <tr>
	<td><a href="https://github.com/iampavangandhi/TheNodeCourse"><b>SUM_GAN</b></a></td>
        <td>Incorporate KoBERT into NMT Task and compare it with BERT-Multilingual</td>
        <td>Korean-English Translation Dataset(AI Hub)</td>
        <td>Sequence-to-Sequence, BERT, KoBERT, Transformer</td>
      </tr>
      <tr>
	<td><a href="https://github.com/iampavangandhi/iampavangandhi"><b>SUM_NMT</b></a></td>
        <td>Apply Adversarial Training via RL technique</td>
        <td>WMT-14(De-En)</td>
        <td>Sequence-to-Sequence, BERT, Transformer</td>
      </tr>
    </tbody>
  </table>
  <br />
</details>


<details>	
  <summary><b> ‚≠ï Dialogue Generation </b></summary><br>
  <table>
    <thead align="center">
      <tr border: none;>
        <td><b>üíª Repo</b></td>
        <td><b>üí° Desc</b></td>
        <td><b>üíæ Dataset</b></td>
        <td><b>‚öôÔ∏è Architecture</b></td>
      </tr>
    </thead>
    <tbody>
      <tr>
	<td><a href="https://github.com/iampavangandhi/Gitwar"><b>Chat_Basics</b></a></td>
        <td>Basic Architectures for machine Translation</td>
        <td>WMT-14(De-En)</td>
        <td>Sequence-to-Sequence, Attention Mechanism, Transformer</td>
      </tr>
      <tr>
	<td><a href="https://github.com/iampavangandhi/TradeByte"><b>Chat_BERT</b></a></td>
        <td>Incorporate BERT into NMT Task</td>
        <td>WMT-14(De-En)</td>
        <td>Sequence-to-Sequence, BERT, Transformer</td>
      </tr>
      <tr>
	<td><a href="https://github.com/iampavangandhi/TheNodeCourse"><b>C2_Bot</b></a></td>
        <td>Characteristic Chat (C2) Bot via GAN</td>
        <td>HIMYM Transcripts</td>
        <td>Sequence-to-Sequence, BERT, KoBERT, Transformer</td>
      </tr>
      <tr>
	<td><a href="https://github.com/iampavangandhi/iampavangandhi"><b>C3_Bot</b></a></td>
        <td>Coherent Characteristic Chat (C3) Bot</td>
        <td>WMT-14(De-En)</td>
        <td>Sequence-to-Sequence, BERT, Transformer</td>
      </tr>
    </tbody>
  </table>
  <br />
</details>


<details>	
  <summary><b> ‚≠ï Ablation Studies </b></summary><br>
  <table>
    <thead align="center">
      <tr border: none;>
        <td><b>üíª Repo</b></td>
        <td><b>üí° Desc</b></td>
        <td><b>üíæ Dataset</b></td>
        <td><b>‚öôÔ∏è Architecture</b></td>
      </tr>
    </thead>
    <tbody>
      <tr>
	<td><a href="https://github.com/iampavangandhi/Gitwar"><b>Transformer_Ablation</b></a></td>
        <td>Basic Architectures for machine Translation</td>
        <td>WMT-14(De-En)</td>
        <td>Sequence-to-Sequence, Attention Mechanism, Transformer</td>
      </tr>
      <tr>
	      <td><a href="https://github.com/iampavangandhi/TradeByte"><b>BERT_Ablation</b></a></td>
        <td>Incorporate BERT into NMT Task</td>
        <td>WMT-14(De-En)</td>
        <td>Sequence-to-Sequence, BERT, Transformer</td>
      </tr>
      <tr>
	<td><a href="https://github.com/iampavangandhi/TheNodeCourse"><b>PipiLine_Ablation</b></a></td>
        <td>Incorporate KoBERT into NMT Task and compare it with BERT-Multilingual</td>
        <td>WMT-14(De-En)</td>
        <td>Transformer</td>
      </tr>
      <tr>
	<td><a href="https://github.com/iampavangandhi/iampavangandhi"><b>Enc_Dec_Ablation</b></a></td>
        <td>Encoding and Decoding Techniques</td>
        <td>WMT-14(De-En)</td>
        <td>Sequence-to-Sequence, Transformer</td>
      </tr>
    </tbody>
  </table>
  <br />
</details>


<details>	
  <summary><b> ‚≠ï Model Lightening </b></summary><br>

| **Name** | **Desc** | **Key Words** |
|------|------|--------|
| **[NLG_Basics](https://github.com/moon23k/NLG_Basics)** | Implements Basic NLG Model Architectures from scratch and Compares performances | Seq2Seq, Attention Mechanism, Transformer, NMT, Dialogue Genreration |
| **[Light Transformer](https://github.com/moon23k/Light_Transformer)** | Compare Performance and Speed between Vanila Transformer and Light-Weighted Transformer | Transformer, Weight-Sharing, Parameter Factorizing, Model Lightening |
| **[NLG_BERT](https://github.com/moon23k/NLG_BERT)** | Apply Pre-trained BERT Model to NLG Tasks | BERT, NMT, Dialogue Genreration |
| **[NLG_KoBERT](https://github.com/moon23k/NMT_KoBERT)** | Compare KoBERT and Multi-Lingual Bert Models on Korean NLG datasets | KoBERT, BERT_multilingual, NMT, Dialogue Genreration |
  <br />
</details>


<details>
  <summary><b> ‚≠ï Datasets </b></summary>

* **[NMT_Basics](https://github.com/moon23k/NLG_Basics)**: Implements Basic NLG Model Architectures from scratch and Compares performances<br>

* **[NMT_BERT](https://github.com/moon23k/NLG_Basics)**: Implements Basic NLG Model Architectures from scratch and Compares performances<br>

* **[NMT_KoBERT](https://github.com/moon23k/NLG_Basics)**: Implements Basic NLG Model Architectures from scratch and Compares performances<br>

* **[NMT_GAN](https://github.com/moon23k/NLG_Basics)**: Implements Basic NLG Model Architectures from scratch and Compares performances<br>
  <br />
</details>




<br>
<br>


### Github Stats  
<table><tr><td valign="top" width="50%">

<img src="https://github-readme-stats.vercel.app/api?username=moon23k&show_icons=true&count_private=true&hide_border=true" align="left" style="width: 100%" />

</td><td valign="top" width="50%">
  
<img src="https://github-readme-stats.vercel.app/api/top-langs/?username=moon23k&hide_border=true&layout=compact" align="left" style="width: 100%" />

</td></tr></table>  
