## <img src="https://emojis.slackmojis.com/emojis/images/1531849430/4246/blob-sunglasses.gif?1531849430" width="30"/> Hello World
Hi there! I'm moon, somebody who's focusing on solving problems through artificial intelligence. AI has many subcategories, of which I find natural language processing the most interesting. By profession, I'm a NLP Machine Learning Engineer. As an engineer, I aim to develop a model that can communicate naturally with people. So the codes in all my repos will contain the progress towards the goal.

<br>

## üë®üèª‚Äçüî¨ Experiments
AI has a lot of variables, which means that different results can be derived depending on the settings of various variables. Below is a series of experiments that introduce different approaches for each task and interpret the results.

<br>

**Neural Machine Translation**
* <a href="https://github.com/moon23k/NMT_Basics">Apply BERT on NMT Task</a>
* <a href="https://github.com/moon23k/NMT_Basics">Apply Korean-BERT on NMT Task and compare it with BERT-MultiLingual Model</a>
* <a href="https://github.com/moon23k/NMT_Basics">Apply GAN through Reinforcement Learning Techniques on NMT Task</a>

<br>

**Abstract Text Summarization**
* <a href="https://github.com/moon23k/NMT_Basics">Hierarchical Seq2Seq</a>
* <a href="https://github.com/moon23k/NMT_Basics">Hierarchical Transformer</a>
* <a href="https://github.com/moon23k/NMT_Basics">Hierarchical BERT</a>
* <a href="https://github.com/moon23k/NMT_Basics">SUM GAN</a>
* <a href="https://github.com/moon23k/NMT_Basics">SUM NMT</a>

<br>

**Dialogue Generation**
* <a href="https://github.com/moon23k/NMT_Basics">Chat BERT</a>
* <a href="https://github.com/moon23k/NMT_Basics">C2 Bot</a>
* <a href="https://github.com/moon23k/NMT_Basics">C2 Bot</a>

<br>

**Ablation Studies**
* <a href="https://github.com/moon23k/NMT_Basics">Transformer Albations</a>
* <a href="https://github.com/moon23k/NMT_Basics">BERTs Albations</a>
* <a href="https://github.com/moon23k/NMT_Basics">Encoder and Decoder Albations</a>
* <a href="https://github.com/moon23k/NMT_Basics">Module PipeLine Albations</a>


<br>

## ‚öì Anchor Codes
Recently, research on AI has been actively conducted, is currently in progress, and there will be more in the future. Accordingly, many new architectures and techniques are proposed. Therefore, a baseline is needed to objectively evaluate performance while applying a new method to existing tasks. I call this baseline code anchor code, and set the three most basic and frequently used architectures in NLP as anchors. Each is Sequence-to-Sequence, Attention Architecture and Transformer. The anchor codes for each representative task of NLP is as follows.

* <a href="https://github.com/moon23k/NMT_Basics">NMT_Basics</a>
* <a href="https://github.com/moon23k/SUM_Basics">SUM_Basics</a>
* <a href="https://github.com/moon23k/Chat_Basics">Chat_Basics</a>

<br>


## üíæ Data

* <a href="https://github.com/moon23k/NMT_Basics">Download and Process</a>
* <a href="https://github.com/moon23k/NMT_Basics">Tokenizer Ablations</a>
