<br> 

## âœ‹ Hello&hairsp;  World
&nbsp; Hi there! I'm **moon**, someone dedicated to tackling challenges through artificial intelligence. Among the various AI subcategories, Natural Language Processing intrigues me the most. By profession, I'm a **NLP Machine Learning Engineer**. My goal as an engineer is to create models that can engage in natural communication with people. The code in all my repositories reflects my progress toward achieving this objective. In addition to the codes in my git repos, I've provided comprehensive project summaries on my <a href="https://moon23k.github.io/">portfolio website</a>. I encourage you to take a moment to explore and discover more about each project. If you'd like to get in touch, feel free to reach out to me via the email address listed in the left information field.

<br><br> 


## ðŸ¤–&hairsp; Model&hairsp;  Architecture 
> &nbsp; The model architecture plays a pivotal role in machine learning engineering, as it can greatly influence performance.
Below, you'll find a collection of projects focused on exploring and establishing standards for appropriate model structures in three NLG tasks: Translation, Dialogue Generation, and Summarization.

&emsp; [**â€¢&hairsp; RNN Seq2Seq**](https://github.com/moon23k/RNN_Seq2Seq) &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &ensp; &emsp; 
       [**â€¢&hairsp; RNN Seq2Seq with Attention**](https://github.com/moon23k/RNN_Seq2Seq_Attention) &emsp; &emsp; &emsp; &emsp; &emsp; 
       [**â€¢&hairsp; Transformer**](https://github.com/moon23k/Transformer) <br> 
       
&emsp; [**â€¢&hairsp; Transformer Balance**](https://github.com/moon23k/Transformer_Balance) &emsp; &emsp; &emsp; &emsp; &emsp; 
       [**â€¢&hairsp; Transformer Variants**](https://github.com/moon23k/Transformer_Variants) &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &ensp; &hairsp; 
       [**â€¢&hairsp; Transformer Fusion**](https://github.com/moon23k/Transformer_Fusion)
       
<br><br> 


## ðŸƒâ€â™‚ï¸&hairsp; Training&hairsp;  Strategy
> &nbsp; Alongside model architecture, another crucial factor influencing the performance of deep learning models is the **training strategy**. In order to explore more advanced training methodologies, various approaches such as pretraining, finetuning, GANs, etc., are being applied and developed, alongside diverse research endeavors including deep studies aimed at enhancing efficiency. 

&emsp; [**â€¢&hairsp; Customized&hairsp; Pretraining**](https://github.com/moon23k/Customized_Pretraining) &emsp; &emsp; &emsp; 
       [**â€¢&hairsp; Generation&hairsp; Improveing&hairsp; Fine-Tuning**](https://github.com/moon23k/GIFT) &emsp; &emsp; &emsp; 
       [**â€¢&hairsp; IntelliGEN**](https://github.com/moon23k/IntelliGEN) 

<br><br> 


## ðŸŽ¯&hairsp; Task Specific&hairsp;  Experiment
> &nbsp; Machine translation is the task of converting Text from Source Language into Target Language using a computer processing. The hegemony of machine translation was Rule-Based at the earliest, followed by SMT, and now NMT has been established. NMT aims to derive more accurate and natural translation results using Neural Networks. Below are experiments of various Neural Network Architectures for this purpose.

&emsp; [**â€¢&hairsp; Multi-Lingual Translation**](https://github.com/moon23k/MultiModal_Translation) &emsp; &emsp; &emsp; 
       [**â€¢&hairsp; Multi-Turn Dialogue Generation**](https://github.com/moon23k/MultiTurn_Dialogue) &emsp; &emsp; &emsp; 
       [**â€¢&hairsp; Efficient Text Summarization**](https://github.com/moon23k/Efficient_Summarization) <br>  
       
<br><br> 


## ðŸ—ï¸&hairsp; LLM&hairsp; Framework
> &nbsp; The Large Language Model (LLM) is currently achieving remarkable results across various fields. However, to fully leverage the outstanding performance of LLM, it is crucial to consider appropriate methodologies tailored to user needs. Even with the basic LLM, significant results can be obtained, but better outcomes can be achieved through slight improvements and additional approaches. In the following projects, methodologies for effectively leveraging LLM according to user needs will be explored, along with the proposal of a practical framework applicable to real-world services.


&emsp; [**â€¢&hairsp; Context-Aware Translation Framework**](https://github.com/moon23k/Context_Framework) &emsp; &emsp; 
       [**â€¢&hairsp; Characterstic Conv Framework**](https://github.com/moon23k/Character_Framework) &emsp; &emsp; 
       [**â€¢&hairsp; Trustworthy Conv Framework**](https://github.com/moon23k/Trust_Framework) <br>  
       
<br><br> 


## ðŸ’¾&hairsp; Dataset 
> &nbsp; Machine translation is the task of converting Text from Source Language into Target Language using a computer processing. The hegemony of machine translation was Rule-Based at the earliest, followed by SMT, and now NMT has been established. NMT aims to derive more accurate and natural translation results using Neural Networks. Below are experiments of various Neural Network Architectures for this purpose.

&emsp; [**â€¢&hairsp; NLP Datasets**](https://github.com/moon23k/NLP_Datasets) &emsp; &emsp; &emsp; &emsp; 
       [**â€¢&hairsp; Tokenizers**](https://github.com/moon23k/Tokenizers) &emsp; &emsp; &emsp; &emsp; 
       [**â€¢&hairsp; Back Translation**](https://github.com/moon23k/BackTranslation) &emsp; &emsp; &emsp; &emsp;
       [**â€¢&hairsp; SemEnt**](https://github.com/moon23k/SemEnt)
<br> 
