## <img src="https://emojis.slackmojis.com/emojis/images/1531849430/4246/blob-sunglasses.gif?1531849430" width="30"/> Hello World
Hi there! I'm moon, somebody who's focusing on solving problems through artificial intelligence. AI has many subcategories, of which I find Natural Language Processing the most interesting. By profession, I'm a NLP Machine Learning Engineer. As an engineer, I aim to develop a model that can communicate naturally with people. So the codes in all my repos will contain the progress towards the goal. In addition to the code, reviews of the papers and personal research on artificial intelligence techniques are recorded on my <a href="">notion page</a>.

<br>

## üë®üèª‚Äçüî¨ Experiments
AI has a lot of variables, which means that different results can be derived depending on the settings of various variables. Therefore, many experiments from various viewpoints are required to obtain good results. Below is a series of experiments that introduce different approaches for three representative NLP tasks and Ablation Studies. 

<br>

**Neural Machine Translation**
* <a href="https://github.com/moon23k/NMT_Basics">How much performance will be improved if a large-scale pre-training model is applied to a machine translation task?</a>
* <a href="https://github.com/moon23k/NMT_Basics">How different is the multilingual pre-trained model and the Korean pre-trained model on Korean Dataset?</a>
* <a href="https://github.com/moon23k/NMT_Basics">How can application of GAN through reinforcement learning technique improve translation quality?</a>

<br>

**Abstract Text Summarization**

> Text Summarization summarizes long text into short sentences through Neural Networks, and the task has Extractive and Abstractive methods. Extractive Summarization selects key sentences from original text to make summary, whereas Abstractive Summarization creates a new summary sentence through the model's decoder. The experiments below mainly deal with Abstractive summary tasks.


* <a href="https://github.com/moon23k/NMT_Basics">Will the hierarchical model structure help to properly understand long sentences?</a>
* <a href="https://github.com/moon23k/NMT_Basics">How helpful is the recursive transformer structure for dealing with long sentences?</a>
* <a href="https://github.com/moon23k/NMT_Basics">How helpful can a large-scale pre-training model and hierarchical structure be to improve performance?</a>
* <a href="https://github.com/moon23k/NMT_Basics">How can application of GAN through reinforcement learning technique improve Summarization Task?</a>

<br>

**Dialogue Generation**
> In general, the dialog generation model tends to give general and repetitive answers independent of the flow of the conversation. This is because it is helpful to generate generic and repetitive answers in a way to reduce the loss in the learning process. Below is a set of experiments to address this and generate a more natural answer.

* <a href="https://github.com/moon23k/NMT_Basics">Application BERT model on Dialogue Generation Task would help to generate natural utterance?</a>
* <a href="https://github.com/moon23k/NMT_Basics">Can application of GAN throgh Policy Graident Technique be helpful to give Characteristic on Dialogue Generation Model?</a>
* <a href="https://github.com/moon23k/NMT_Basics">Can Hierarchical Model Structure and GAN make Coherent Characteristic Chatbot?</a>

<br>

**Ablation Studies**
> AI models show different results depending on the architecture and different techniques.

* <a href="https://github.com/moon23k/NMT_Basics">Transformer Albations</a>
* <a href="https://github.com/moon23k/NMT_Basics">Albation studies on BERT based models</a>
* <a href="https://github.com/moon23k/NMT_Basics">Encoder and Decoder Albations</a>
* <a href="https://github.com/moon23k/NMT_Basics">Module PipeLine Albations</a>


<br>

## ‚öì Anchor Codes
Recently, research on AI has been actively conducted, is currently in progress, and there will be more in the future. Accordingly, many new architectures and techniques are proposed. Therefore, a baseline is needed to objectively evaluate performance while applying a new method to existing tasks. I call this baseline code anchor code, and set the three most basic and frequently used architectures in NLP as anchors. Each is **Sequence-to-Sequence**, **Attention Architecture** and **Transformer**. The anchor codes for each representative task of NLP is as follows.

* <a href="https://github.com/moon23k/NMT_Basics">Neural Machine Translation Anchor Codes</a>
* <a href="https://github.com/moon23k/SUM_Basics">Abstractive Text Summarization Anchor Codes</a>
* <a href="https://github.com/moon23k/Chat_Basics">Dialogue Generation Anchor Codes</a>

<br>


## üíæ Data
AI models learn from large amounts of data. Therefore, high-quality bulk data is essential for building a good model. Research on data explores the series of processes of importing data and processing it into a form suitable for model learning.

* <a href="https://github.com/moon23k/NMT_Basics">Download and Process</a>
* <a href="https://github.com/moon23k/NMT_Basics">Tokenizer Ablations</a>
* <a href="https://github.com/moon23k/NMT_Basics">Data Augmentation</a>
