<br> 

## âœ‹ Hello World
&nbsp; Hi there! I'm **moon**, someone dedicated to tackling challenges through artificial intelligence. Among the various AI subcategories, Natural Language Processing intrigues me the most. By profession, I'm a **NLP Machine Learning Engineer**. My goal as an engineer is to create models that can engage in natural communication with people. The code in all my repositories reflects my progress toward achieving this objective. In addition to the codes in my git repos, I've provided comprehensive project summaries on my <a href="https://moon23k.github.io/">portfolio website</a>. I encourage you to take a moment to explore and discover more about each project. If you'd like to get in touch, feel free to reach out to me via the email address listed in the left information field.

<br><br> 


## ðŸ¤– Model Architecture
> &nbsp; The model architecture plays a pivotal role in machine learning engineering, as it can greatly influence performance.
Below, you'll find a collection of projects focused on exploring and establishing standards for appropriate model structures in three NLG tasks: Translation, Dialogue Generation, and Summarization.

&emsp; [**â€¢ &hairsp; RNN Seq2Seq**](https://github.com/moon23k/RNN_Seq2Seq) &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &ensp; &ensp; &nbsp; 
       [**â€¢ &hairsp; RNN Seq2Seq with Attention**](https://github.com/moon23k/RNN_Seq2Seq_Attention) &emsp; &emsp; &emsp; &emsp; &emsp; 
       [**â€¢ &hairsp; Transformer**](https://github.com/moon23k/Transformer) <br><br> 
&emsp; [**â€¢ &hairsp; Transformer Variants**](https://github.com/moon23k/Transformer_Variants) &emsp; &emsp; &emsp; &emsp; &emsp; 
       [**â€¢ &hairsp; Transformer Balance**](https://github.com/moon23k/Transformer_Balance) &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; 
       [**â€¢ &hairsp; Transformer Fusion**](https://github.com/moon23k/Transformer_Fusion)
       
<br><br> 


## ðŸƒâ€â™‚ï¸ Training Strategy
> &nbsp; Alongside model architecture, another crucial factor influencing the performance of deep learning models is the **training strategy**. Below, you'll find five research projects aimed at enhancing performance to bring out the best in model capabilities.

&emsp; [**â€¢ &hairsp; Auxiliary Training**](https://github.com/moon23k/Aux_Train) &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; 
       [**â€¢ &hairsp; Scheduled Sampling**](https://github.com/moon23k/Scheduled_Sampling) &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; 
       [**â€¢ &hairsp; Customized PreTraining**](https://github.com/moon23k/CPT_Train) <br><br> 
&emsp; [**â€¢ &hairsp; Generative Training**](https://github.com/moon23k/GEN_Train) &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &hairsp; 
       [**â€¢ &hairsp; SeqGAN**](https://github.com/moon23k/GAN_Train)

<br><br> 


## â° Toward Efficiency
> &nbsp; Large-scale models with numerous parameters are known to yield improved performance. However, deploying such extensive models in typical computing environments can be limiting. To tackle this challenge, the following project introduces an efficient approach that ensures a certain level of performance while alleviating computational demands.




&emsp; [**â€¢ &hairsp; Efficient Training**](https://github.com/moon23k/Efficient_Training) &emsp; &emsp; &emsp; &emsp; &emsp; 
       [**â€¢ &hairsp; Efficient PreTrained Language Models**](https://github.com/moon23k/Efficient_PLMs) &emsp; &emsp; &emsp; &emsp; &emsp; 
       [**â€¢ &hairsp; Param Efficient Fine-Tuning**](https://github.com/moon23k/PEFT)

<br><br> 


## ðŸ”„ Neural Machine Translation
> &nbsp; Machine translation is the task of converting Text from Source Language into Target Language using a computer processing. The hegemony of machine translation was Rule-Based at the earliest, followed by SMT, and now NMT has been established. NMT aims to derive more accurate and natural translation results using Neural Networks. Below are experiments of various Neural Network Architectures for this purpose.

&emsp; [**â€¢ &hairsp; Back Translation**](https://github.com/moon23k/NMT_Back) &emsp; &emsp; &emsp; &emsp; &emsp; 
       [**â€¢ &hairsp; Multi-Lingual Translation**](https://github.com/moon23k/NMT_MultiLingual) &emsp; &emsp; &emsp; &emsp; &emsp; 
       [**â€¢ &hairsp; Code Translation**](https://github.com/moon23k/NMT_Code) 

<br><br> 


## ðŸ—£ï¸ Dialogue Generation
> &nbsp; Dialogue Generation is a task to generate a response to a previous utterance, just like humans do in a conversational situation. However, it is very difficult for the model to understand the flow of the conversation and return appropriate answers. Below are a set of experiments to generate more natural responses like humans do.

&emsp; [**â€¢ &hairsp; Characteristic Dialogue**](https://github.com/moon23k/Dialog_Char) &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; 
       [**â€¢ &hairsp; Utilize SimEnt**](https://github.com/moon23k/Dialog_SimEnt) 

<br><br> 


## ðŸ“ Abstract Text Summarization
> &nbsp; Summarization Task summarizes long text into short sentences through Neural Networks, and the task can be devided into Extractive and Abstractive methods. Extractive Summarization selects key sentences from original text to make summary, whereas Abstractive Summarization creates a new summary sentence through the model's decoder. The experiments below mainly deal with Abstractive summary tasks.

&emsp; [**â€¢ Hierarchical Encoder**](https://github.com/moon23k/Summ_HierEnc) &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; 
       [**â€¢ Sparse Attention**](https://github.com/moon23k/Summ_Sparse) 

<br> 
